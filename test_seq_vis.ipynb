{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchvision\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, \"../utils\")\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import src.datasets.cityscapes_loader as cityscapes_loader\n",
    "import utils.train_eval as train_eval\n",
    "import importlib\n",
    "import visualizations as vis\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2975 train images\n",
      "Found 500 val images\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(cityscapes_loader)\n",
    "\n",
    "is_sequence = True\n",
    "\n",
    "dataset_root_dir = \"/home/nfs/inf6/data/datasets/cityscapes/\"\n",
    "\n",
    "train_ds = cityscapes_loader.cityscapesLoader(root=dataset_root_dir, split='train', img_size=(512, 1024), is_transform=True, is_sequence=is_sequence)\n",
    "val_ds = cityscapes_loader.cityscapesLoader(root=dataset_root_dir, split='val', img_size=(512, 1024), is_transform=True, is_sequence=is_sequence)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=2, shuffle=True, drop_last=True)\n",
    "valid_loader = torch.utils.data.DataLoader(val_ds, batch_size=1, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/sheludzk/CudaLab_Project/tboard_logs/Temporal_ResUNetConfig_Conv2dGRUCell/Layers4_InitDim16\n"
     ]
    }
   ],
   "source": [
    "from src.architectures.architecture_configs import *\n",
    "import src.architectures.Temporal_UNET_Template as Temporal_UNET_Template\n",
    "import utils.utils\n",
    "\n",
    "encoder_blocks = SmallDeep_NetworkSize.encoder_blocks\n",
    "decoder_blocks = SmallDeep_NetworkSize.decoder_blocks\n",
    "\n",
    "config = Temporal_ResUNetConfig(\n",
    "    encoder_blocks=encoder_blocks,\n",
    "    decoder_blocks=decoder_blocks,\n",
    "    temporal_cell= Conv2dGRUCell\n",
    "    )\n",
    "\n",
    "temp_unet = Temporal_UNET_Template.Temporal_UNet(config)\n",
    "\n",
    "temp_unet_optim = torch.optim.Adam(temp_unet.parameters(), lr=3e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs=100\n",
    "temp_unet_trainer = utils.train_eval.Trainer(\n",
    "            temp_unet, temp_unet_optim, criterion,\n",
    "            train_loader, valid_loader, \"cityscapes\", epochs,\n",
    "            sequence=True, all_labels=20, start_epoch=17)\n",
    "\n",
    "load_model = True\n",
    "if load_model:\n",
    "    temp_unet_trainer.load_model(\"cityscapes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 20, 512, 1024])\n",
      "torch.Size([1, 12, 20, 512, 1024])\n",
      "torch.Size([1, 12, 20, 512, 1024])\n",
      "torch.Size([1, 12, 20, 512, 1024])\n",
      "torch.Size([1, 12, 20, 512, 1024])\n",
      "torch.Size([1, 12, 20, 512, 1024])\n",
      "torch.Size([1, 12, 20, 512, 1024])\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "@torch.no_grad()\n",
    "def vis_seq(model, loader):\n",
    "    visualizer = vis.CityscapesVisualizer()\n",
    "    model.eval()\n",
    "    for i, (imgs, targets) in enumerate(loader):\n",
    "        imgs = imgs.cuda()\n",
    "        preds = model(imgs)\n",
    "        print(preds.shape)\n",
    "        for i in range(preds.shape[0]):\n",
    "            decoded_seq = get_decoded_img_seq(preds[i])\n",
    "\n",
    "            for j in range(len(decoded_seq)):\n",
    "                plt.imshow(decoded_seq[j])\n",
    "                plt.show()\n",
    "\n",
    "            break\n",
    "        break\n",
    "    return \n",
    "\n",
    "@torch.no_grad()\n",
    "def save_vis_seq(model, loader, model_name=\"default\"):\n",
    "    if not os.path.exists(\"imgs\"):\n",
    "        os.makedirs(\"imgs\")\n",
    "    if not os.path.exists(f\"imgs/{model_name}\"):\n",
    "        os.makedirs(f\"imgs/{model_name}\")\n",
    "\n",
    "    visualizer = vis.CityscapesVisualizer()\n",
    "    model.eval()\n",
    "    for k, (imgs, targets) in enumerate(loader):\n",
    "        if not os.path.exists(f\"imgs/{model_name}/{k}\"):\n",
    "            os.makedirs(f\"imgs/{model_name}/{k}\")\n",
    "        if not os.path.exists(f\"imgs/{model_name}/{k}/original\"):\n",
    "            os.makedirs(f\"imgs/{model_name}/{k}/original\")\n",
    "        if not os.path.exists(f\"imgs/{model_name}/{k}/predicted\"):\n",
    "            os.makedirs(f\"imgs/{model_name}/{k}/predicted\")\n",
    "        imgs = imgs.cuda()\n",
    "        preds = model(imgs)\n",
    "        print(preds.shape)\n",
    "        for i in range(preds.shape[0]):\n",
    "\n",
    "            decoded_seq = get_decoded_img_seq(preds[i])\n",
    "\n",
    "            for j in range(len(decoded_seq)):\n",
    "                torchvision.utils.save_image(torch.from_numpy(decoded_seq[j].transpose(2,0,1)), os.path.join(os.getcwd(), \"imgs\", f\"{model_name}\", f\"{k}\", \"predicted\", f\"imgs_{j}.png\"))\n",
    "\n",
    "                mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
    "                std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
    "                unnorm_imgs = imgs.cpu() * std + mean\n",
    "\n",
    "                torchvision.utils.save_image(unnorm_imgs[i, j], os.path.join(os.getcwd(), \"imgs\", f\"{model_name}\", f\"{k}\", \"original\", f\"imgs_{j}.png\"))\n",
    "\n",
    "        if k > 5:\n",
    "            break\n",
    "\n",
    "    return \n",
    "\n",
    "def get_decoded_img_seq(preds):\n",
    "    result = []\n",
    "    visualizer = vis.CityscapesVisualizer()\n",
    "    predicted_class = torch.argmax(preds, dim=1)\n",
    "    for j in range(preds.shape[0]):\n",
    "        decoded_pred = visualizer.decode_segmap(predicted_class[j].cpu().numpy())\n",
    "        result.append(decoded_pred)\n",
    "        #torchvision.utils.save_image(torch.from_numpy(decoded_pred.transpose(2,0,1)), os.path.join(os.getcwd(), \"imgs\", \"training\", f\"imgs_{j}.png\"))\n",
    "    return result\n",
    "\n",
    "save_vis_seq(temp_unet, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import PIL\n",
    "import os\n",
    "\n",
    "def create_gifs(model_name=\"default\", mode=\"side-by-side\", transparency=0.5, fps=8):\n",
    "\n",
    "    allowed_modes = [\"side-by-side\", \"overlay\"]\n",
    "    if mode not in allowed_modes:\n",
    "        raise ValueError(f\"mode must be one of {alloud_modes}\")\n",
    "    imgs_root=f\"imgs/{model_name}\"\n",
    "    dirlist = [ item for item in os.listdir(imgs_root) if os.path.isdir(os.path.join(imgs_root, item)) ]\n",
    "    # remove \"gifs\"-folder from dirlist\n",
    "    dirlist = [item for item in dirlist if item != \"gifs\"]\n",
    "\n",
    "    if not os.path.exists(f\"imgs/{model_name}/gifs\"):\n",
    "        os.makedirs(f\"imgs/{model_name}/gifs\")\n",
    "\n",
    "\n",
    "\n",
    "    if mode == \"side-by-side\":\n",
    "        for i in range(len(dirlist)):\n",
    "            images = []\n",
    "            for j in range(12):\n",
    "                original = PIL.Image.open(f\"imgs/{model_name}/{dirlist[i]}/original/imgs_{j}.png\")\n",
    "                prediction = PIL.Image.open(f\"imgs/{model_name}/{i}/predicted/imgs_{j}.png\")\n",
    "\n",
    "                (width1, height1) = original.size\n",
    "                (width2, height2) = prediction.size\n",
    "\n",
    "                result_width = width1 + width2\n",
    "                result_height = max(height1, height2)\n",
    "\n",
    "                result = PIL.Image.new('RGB', (result_width, result_height))\n",
    "                result.paste(im=original, box=(0, 0))\n",
    "                result.paste(im=prediction, box=(width1, 0))\n",
    "                images.append(result)\n",
    "            imageio.mimsave(f\"imgs/{model_name}/gifs/{i}.gif\", images, fps=fps)\n",
    "\n",
    "    elif mode == \"overlay\":\n",
    "        for i in dirlist:\n",
    "            images = []\n",
    "            for j in range(12):\n",
    "                background = PIL.Image.open(f\"imgs/{model_name}/{i}/original/imgs_{j}.png\")\n",
    "                foreground  = PIL.Image.open(f\"imgs/{model_name}/{i}/predicted/imgs_{j}.png\")\n",
    "                foreground.putalpha(int(255*(1-transparency))) \n",
    "                background.paste(foreground, (0, 0), mask=foreground)\n",
    "                images.append(background)\n",
    "            imageio.mimsave(f\"imgs/{model_name}/gifs/{i}.gif\", images, fps=fps)\n",
    "\n",
    "create_gifs(\"default\", mode=\"overlay\", transparency=0.45, fps=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
